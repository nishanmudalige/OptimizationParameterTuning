{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a199e49-3ff1-40db-b33e-f3712acb4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/Year3/STA378/OptimizationParameterTuning/Julia Notebook/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/C:\\Users\\tangi\\.julia\\dev\\FelixUoT`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(raw\"C:\\Users\\tangi\\.julia\\dev\\FelixUoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935168d1-78ab-4775-862e-b8b02121a279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Desktop/Year3/STA378/OptimizationParameterTuning/Julia Notebook/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/Project.toml` (empty project)\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedf973f-9a01-4134-9770-8072533eb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/Year3/STA378/OptimizationParameterTuning/Julia Notebook/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/Year3/STA378/OptimizationParameterTuning/Julia Notebook/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "Pkg.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58acd1b-034d-4e0b-b5c6-50bed5704d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Desktop/Year3/STA378/OptimizationParameterTuning/Julia Notebook/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/C:\\Users\\tangi\\.julia\\dev\\FelixUoT/Project.toml` (empty project)\n"
     ]
    }
   ],
   "source": [
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c4460-96bc-4a2e-a892-9b0a92f70773",
   "metadata": {},
   "source": [
    "## Optimization problem modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81ca0ad-0887-420f-a980-71aa21ecbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels # see https://github.com/JuliaSmoothOptimizers/NLPModels.jl, it defines an abstract API to access a continuous optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905091eb-4a3d-477d-afdc-10766d0155db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "AbstractNLPModel\n",
       "\\end{verbatim}\n",
       "Base type for an optimization model.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "AbstractNLPModel\n",
       "```\n",
       "\n",
       "Base type for an optimization model.\n"
      ],
      "text/plain": [
       "\u001b[36m  AbstractNLPModel\u001b[39m\n",
       "\n",
       "  Base type for an optimization model."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@doc AbstractNLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7eb0ad-3fda-450c-8adc-d855e99602f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels # see https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl\n",
    "                  # is a concrete implementation of the ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82152b8d-da8c-4658-a0e6-f374414400f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "ADNLPModel(f, x0)\n",
       "ADNLPModel(f, x0, lvar, uvar)\n",
       "ADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\n",
       "ADNLPModel(f, x0, A, lcon, ucon)\n",
       "ADNLPModel(f, x0, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, A, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\n",
       "ADNLPModel(model::AbstractNLPModel)\n",
       "\\end{verbatim}\n",
       "ADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n",
       "\n",
       "\\begin{verbatim}\n",
       " min  f(x)\n",
       "s.to  lcon ≤ (  Ax  ) ≤ ucon\n",
       "             ( c(x) )\n",
       "      lvar ≤   x  ≤ uvar.\n",
       "\\end{verbatim}\n",
       "The following keyword arguments are available to all constructors:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{minimize}: A boolean indicating whether this is a minimization problem (default: true)\n",
       "\n",
       "\n",
       "\\item \\texttt{name}: The name of the model (default: \"Generic\")\n",
       "\n",
       "\\end{itemize}\n",
       "The following keyword arguments are available to the constructors for constrained problems:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{y0}: An inital estimate to the Lagrangian multipliers (default: zeros)\n",
       "\n",
       "\\end{itemize}\n",
       "\\texttt{ADNLPModel} uses \\texttt{ForwardDiff} and \\texttt{ReverseDiff} for the automatic differentiation. One can specify a new backend with the keyword arguments \\texttt{backend::ADNLPModels.ADBackend}. There are three pre-coded backends:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item the default \\texttt{ForwardDiffAD}.\n",
       "\n",
       "\n",
       "\\item \\texttt{ReverseDiffAD}.\n",
       "\n",
       "\n",
       "\\item \\texttt{ZygoteDiffAD} accessible after loading \\texttt{Zygote.jl} in your environment.\n",
       "\n",
       "\\end{itemize}\n",
       "For an advanced usage, one can define its own backend and redefine the API as done in \\href{https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl/blob/main/src/forward.jl}{ADNLPModels.jl/src/forward.jl}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "nvar = 3\n",
       "ADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\n",
       "ADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n",
       "\n",
       "using Zygote\n",
       "ADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "c(x) = [1x[1] + x[2]; x[2]]\n",
       "nvar, ncon = 3, 2\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n",
       "\n",
       "using Zygote\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n",
       "\\end{verbatim}\n",
       "For in-place constraints function, use one of the following constructors:\n",
       "\n",
       "\\begin{verbatim}\n",
       "ADNLPModel!(f, x0, c!, lcon, ucon)\n",
       "ADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\n",
       "ADNLPModel!(f, x0, A, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\n",
       "ADNLSModel!(model::AbstractNLSModel)\n",
       "\\end{verbatim}\n",
       "where the constraint function has the signature \\texttt{c!(output, input)}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "function c!(output, x) \n",
       "  output[1] = 1x[1] + x[2]\n",
       "  output[2] = x[2]\n",
       "end\n",
       "nvar, ncon = 3, 2\n",
       "nlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "ADNLPModel(f, x0)\n",
       "ADNLPModel(f, x0, lvar, uvar)\n",
       "ADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\n",
       "ADNLPModel(f, x0, A, lcon, ucon)\n",
       "ADNLPModel(f, x0, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, A, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\n",
       "ADNLPModel(model::AbstractNLPModel)\n",
       "```\n",
       "\n",
       "ADNLPModel is an AbstractNLPModel using automatic differentiation to compute the derivatives. The problem is defined as\n",
       "\n",
       "```\n",
       " min  f(x)\n",
       "s.to  lcon ≤ (  Ax  ) ≤ ucon\n",
       "             ( c(x) )\n",
       "      lvar ≤   x  ≤ uvar.\n",
       "```\n",
       "\n",
       "The following keyword arguments are available to all constructors:\n",
       "\n",
       "  * `minimize`: A boolean indicating whether this is a minimization problem (default: true)\n",
       "  * `name`: The name of the model (default: \"Generic\")\n",
       "\n",
       "The following keyword arguments are available to the constructors for constrained problems:\n",
       "\n",
       "  * `y0`: An inital estimate to the Lagrangian multipliers (default: zeros)\n",
       "\n",
       "`ADNLPModel` uses `ForwardDiff` and `ReverseDiff` for the automatic differentiation. One can specify a new backend with the keyword arguments `backend::ADNLPModels.ADBackend`. There are three pre-coded backends:\n",
       "\n",
       "  * the default `ForwardDiffAD`.\n",
       "  * `ReverseDiffAD`.\n",
       "  * `ZygoteDiffAD` accessible after loading `Zygote.jl` in your environment.\n",
       "\n",
       "For an advanced usage, one can define its own backend and redefine the API as done in [ADNLPModels.jl/src/forward.jl](https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl/blob/main/src/forward.jl).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "nvar = 3\n",
       "ADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\n",
       "ADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n",
       "\n",
       "using Zygote\n",
       "ADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\n",
       "```\n",
       "\n",
       "```julia\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "c(x) = [1x[1] + x[2]; x[2]]\n",
       "nvar, ncon = 3, 2\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\n",
       "\n",
       "using Zygote\n",
       "ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\n",
       "```\n",
       "\n",
       "For in-place constraints function, use one of the following constructors:\n",
       "\n",
       "```\n",
       "ADNLPModel!(f, x0, c!, lcon, ucon)\n",
       "ADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\n",
       "ADNLPModel!(f, x0, A, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\n",
       "ADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\n",
       "ADNLSModel!(model::AbstractNLSModel)\n",
       "```\n",
       "\n",
       "where the constraint function has the signature `c!(output, input)`.\n",
       "\n",
       "```julia\n",
       "using ADNLPModels\n",
       "f(x) = sum(x)\n",
       "x0 = ones(3)\n",
       "function c!(output, x) \n",
       "  output[1] = 1x[1] + x[2]\n",
       "  output[2] = x[2]\n",
       "end\n",
       "nvar, ncon = 3, 2\n",
       "nlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  ADNLPModel(f, x0)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, clinrows, clincols, clinvals, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, A, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, clinrows, clincols, clinvals, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, A, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, A, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, A, c, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(model::AbstractNLPModel)\u001b[39m\n",
       "\n",
       "  ADNLPModel is an AbstractNLPModel using automatic differentiation to compute\n",
       "  the derivatives. The problem is defined as\n",
       "\n",
       "\u001b[36m   min  f(x)\u001b[39m\n",
       "\u001b[36m  s.to  lcon ≤ (  Ax  ) ≤ ucon\u001b[39m\n",
       "\u001b[36m               ( c(x) )\u001b[39m\n",
       "\u001b[36m        lvar ≤   x  ≤ uvar.\u001b[39m\n",
       "\n",
       "  The following keyword arguments are available to all constructors:\n",
       "\n",
       "    •  \u001b[36mminimize\u001b[39m: A boolean indicating whether this is a minimization\n",
       "       problem (default: true)\n",
       "\n",
       "    •  \u001b[36mname\u001b[39m: The name of the model (default: \"Generic\")\n",
       "\n",
       "  The following keyword arguments are available to the constructors for\n",
       "  constrained problems:\n",
       "\n",
       "    •  \u001b[36my0\u001b[39m: An inital estimate to the Lagrangian multipliers (default:\n",
       "       zeros)\n",
       "\n",
       "  \u001b[36mADNLPModel\u001b[39m uses \u001b[36mForwardDiff\u001b[39m and \u001b[36mReverseDiff\u001b[39m for the automatic\n",
       "  differentiation. One can specify a new backend with the keyword arguments\n",
       "  \u001b[36mbackend::ADNLPModels.ADBackend\u001b[39m. There are three pre-coded backends:\n",
       "\n",
       "    •  the default \u001b[36mForwardDiffAD\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mReverseDiffAD\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mZygoteDiffAD\u001b[39m accessible after loading \u001b[36mZygote.jl\u001b[39m in your\n",
       "       environment.\n",
       "\n",
       "  For an advanced usage, one can define its own backend and redefine the API\n",
       "  as done in ADNLPModels.jl/src/forward.jl\n",
       "  (https://github.com/JuliaSmoothOptimizers/ADNLPModels.jl/blob/main/src/forward.jl).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  using ADNLPModels\u001b[39m\n",
       "\u001b[36m  f(x) = sum(x)\u001b[39m\n",
       "\u001b[36m  x0 = ones(3)\u001b[39m\n",
       "\u001b[36m  nvar = 3\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0) # uses the default ForwardDiffAD backend.\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0; backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  using Zygote\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0; backend = ADNLPModels.ZygoteAD)\u001b[39m\n",
       "\n",
       "\u001b[36m  using ADNLPModels\u001b[39m\n",
       "\u001b[36m  f(x) = sum(x)\u001b[39m\n",
       "\u001b[36m  x0 = ones(3)\u001b[39m\n",
       "\u001b[36m  c(x) = [1x[1] + x[2]; x[2]]\u001b[39m\n",
       "\u001b[36m  nvar, ncon = 3, 2\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ReverseDiffAD) # uses ReverseDiffAD backend.\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  using Zygote\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, c, zeros(ncon), zeros(ncon); backend = ADNLPModels.ZygoteAD)\u001b[39m\n",
       "\n",
       "  For in-place constraints function, use one of the following constructors:\n",
       "\n",
       "\u001b[36m  ADNLPModel!(f, x0, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel!(f, x0, clinrows, clincols, clinvals, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel!(f, x0, A, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, clinrows, clincols, clinvals, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLPModel(f, x0, lvar, uvar, A, c!, lcon, ucon)\u001b[39m\n",
       "\u001b[36m  ADNLSModel!(model::AbstractNLSModel)\u001b[39m\n",
       "\n",
       "  where the constraint function has the signature \u001b[36mc!(output, input)\u001b[39m.\n",
       "\n",
       "\u001b[36m  using ADNLPModels\u001b[39m\n",
       "\u001b[36m  f(x) = sum(x)\u001b[39m\n",
       "\u001b[36m  x0 = ones(3)\u001b[39m\n",
       "\u001b[36m  function c!(output, x) \u001b[39m\n",
       "\u001b[36m    output[1] = 1x[1] + x[2]\u001b[39m\n",
       "\u001b[36m    output[2] = x[2]\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  nvar, ncon = 3, 2\u001b[39m\n",
       "\u001b[36m  nlp = ADNLPModel!(f, x0, c!, zeros(ncon), zeros(ncon)) # uses the default ForwardDiffAD backend.\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@doc ADNLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe11d78-3e3d-4839-9e44-00030c040758",
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems\n",
    "using OptimizationProblems.ADNLPProblems # https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl\n",
    "                                         # contains a test set of problems of type ADNLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18780905-b5e3-46aa-95b6-a156e3257571",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = OptimizationProblems.ADNLPProblems.arglina(matrix_free = true) # is one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574c53d-51b7-4e93-94a8-27d8f14e9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizationProblems.meta # is a DataFrame that allows us to check some information on the test problems without loading them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b669f-455f-48a0-a76b-bcedabd12769",
   "metadata": {},
   "source": [
    "We can also explore the `fieldnames` of a datatype to see what data it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ad25e-b2e9-4e24-a128-d50421b3ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(typeof(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96152178-56a1-47fe-a1e5-d900c1880f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(typeof(nlp.meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f010778-7e10-406d-ab41-4ec1f4b2b5fa",
   "metadata": {},
   "source": [
    "## Optimization problem solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c01d7-0aeb-4a71-83cd-d006855c84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JSOSuite # see https://github.com/JuliaSmoothOptimizers/JSOSuite.jl\n",
    "               # is a wrapper to centralize all the solvers in JuliaSmoothOptimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c55916-7e08-4b20-b3a8-5502e36c328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSOSuite.optimizers # is a DataFrame that present the different solvers and some of their characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f7513-b76c-4e07-9d21-d15bf5c3ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSOSuite.optimizers[JSOSuite.optimizers.name .== \"LBFGS\", :] # In this project, we will focus on L-BFGS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937319d6-f2a7-43d2-8c9c-f8982ffd07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverParameters # Define the main structure to handle the algorithm's parameters\n",
    "using JSOSolvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376fa03-8166-4896-88fa-d43b006cca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_set = JSOSolvers.LBFGSParameterSet(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794aea3c-e8b7-48f8-9094-f5440721220a",
   "metadata": {},
   "source": [
    "By checking the fields inside this structure we see that there are three parameters `mem`, `τ₁` and `bk_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3afe0-32da-4e00-a8dd-67011d79d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(typeof(param_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39e316-3c42-4419-8651-8095ce40c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(param_set.mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de88d94-9258-40da-a270-cb53a6cbb54e",
   "metadata": {},
   "source": [
    "By checking the domain of this parameter, we can see that it must be an integer between 5 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863cbf9-d3fe-4bd0-b171-54baac0de375",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = domain(param_set.mem)\n",
    "typeof(d)\n",
    "supertypes(typeof(d))\n",
    "fieldnames(typeof(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46319346",
   "metadata": {},
   "outputs": [],
   "source": [
    "methodswith(IntegerRange{Int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d5c69-5ce1-4624-bc98-ddcde3fde8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = JSOSolvers.lbfgs(nlp; verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef20c27-92e3-470b-9e16-0e7e9a0dbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore # Define the type GenericExecutionStats that is returned by all solvers\n",
    "fieldnames(typeof(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f027a7-7a9c-4225-9535-7801d607524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc GenericExecutionStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d267f52-478a-483b-a309-4d4d3df9d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc JSOSolvers.lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ed503-8c33-4426-9737-0d36db546957",
   "metadata": {},
   "source": [
    "## Loop over the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92733e-2203-41f2-a60b-cc19c17b12c8",
   "metadata": {},
   "source": [
    "First, we select a subset of the problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c99beb-061f-4308-8b40-2fdca57387a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = OptimizationProblems.meta\n",
    "problem_names = meta[meta.contype .== :unconstrained .&& .!meta.has_bounds .&& meta.nvar .<= 10, :name]; # LBFGS is an algorithm to solve unconstrained problems\n",
    "                                                                   # For this example, we select only problems of size up to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e525eb6-38c6-46ab-83df-7cfeda571c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [Meta.parse(\"OptimizationProblems.ADNLPProblems.eval($problem)()\") for problem ∈ problem_names]; # https://jso.dev/OptimizationProblems.jl/dev/benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c4b49-2915-4002-872c-b93b498b0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames # we will use a DataFrame to store our results\n",
    "df = DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d8e16-1db7-463e-bc79-53a40a9f4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for pb_expr in problems\n",
    "#     nlp = eval(pb_expr)\n",
    "#     i += 1\n",
    "#     @info \"($i / $(length(problems)) Number of variables of problem $(nlp.meta.name) is $(nlp.meta.nvar)\"\n",
    "#     mem = 5\n",
    "#     try\n",
    "#     stats = JSOSolvers.lbfgs(nlp; mem = 5)\n",
    "#     push!(df, (; status = stats.status, name = nlp.meta.name, nvar = nlp.meta.nvar))\n",
    "#     catch e\n",
    "#         @info \"Solver failed on $(nlp.meta.name): $e\"\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7801df-0c43-4181-92e9-01f91f48a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df; # contains the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fd8ec-bc0d-424d-a4b9-59b1556e303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(df[!, :status]) # :first_order is a success, :unbounded too\n",
    "                       # :max_time may indicate that we should run the algorithms with `max_time` greater than default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a946e",
   "metadata": {},
   "source": [
    "## Week1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967adcc",
   "metadata": {},
   "source": [
    "Varying the mem parameters using the domain parameter to get the domain of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "getcount(c, s) = hasfield(typeof(c), s) ? getfield(c, s) : 0\n",
    "\n",
    "vecmat_ops(c) = getcount(c, :neval_grad) +\n",
    "                getcount(c, :neval_jac)  +\n",
    "                getcount(c, :neval_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "using Random\n",
    "using BenchmarkTools\n",
    "\n",
    "meta = OptimizationProblems.meta\n",
    "problem_names = meta[meta.contype .== :unconstrained .&& .!meta.has_bounds .&& meta.nvar .<= 10, :name]; # LBFGS is an algorithm to solve unconstrained problems\n",
    "                                                                   # For this example, we select only problems of size up to 10.\n",
    "problems = [Meta.parse(\"OptimizationProblems.ADNLPProblems.eval($problem)()\") for problem ∈ problem_names]; # https://jso.dev/OptimizationProblems.jl/dev/benchmark/                                                \n",
    "filename = \"./result.csv\"\n",
    "# problems = rand(problem_names, 20)\n",
    "df = DataFrame(\"Problem class\" => String[],\n",
    "                \"Solver\" => String[],\n",
    "                \"mem=\"  => Int[],\n",
    "                \"Runtime\" => Float64[],\n",
    "                \"Memory used\" => Float64[],\n",
    "                \"Number of steps to solve\" => Int[],\n",
    "                \"Number of vector-matrix opeartions\" => Int[]\n",
    "              )\n",
    "CSV.write(filename, df; append=false)\n",
    "i = 0\n",
    "for pb_expr in problems\n",
    "    solver = \"lbfgs\"\n",
    "    lbfgs = eval(pb_expr)\n",
    "    i += 1\n",
    "    param_set = JSOSolvers.LBFGSParameterSet(lbfgs)\n",
    "    r = domain(param_set.mem)\n",
    "    for mem in r.lower:r.upper\n",
    "        println(\"Running $pb_expr with mem=$mem\")\n",
    "        try\n",
    "        stat = JSOSolvers.lbfgs(nlp; mem=mem)\n",
    "        res = @benchmark JSOSolvers.lbfgs($nlp; mem=$mem) evals=1 samples=5\n",
    "        ops = vecmat_ops(nlp.counters)\n",
    "        push!(df, (\"NLP\", solver, mem, minimum(res).time, minimum(res).memory, stat.iter, ops))\n",
    "        catch e\n",
    "            @info \"Solver failed on $(nlp.meta.name): $e\"\n",
    "            # push!(df, (pb_expr, solver, ))\n",
    "            break\n",
    "        end\n",
    "        CSV.write(filename, DataFrame([last(df)]); append=true, header=false)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
